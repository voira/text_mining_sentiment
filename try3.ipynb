{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "\n",
    "# Load dataset\n",
    "def load_dataset(file_path):\n",
    "    reviews, sentiments = [], []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            reviews.append(row[0].lower())\n",
    "            sentiments.append(1 if row[1].strip().lower() == \"positive\" else 0)\n",
    "    return reviews, np.array(sentiments)\n",
    "\n",
    "# Split dataset\n",
    "def split_dataset(reviews, sentiments, train_ratio=0.8):\n",
    "    indices = list(range(len(reviews)))\n",
    "    random.shuffle(indices)\n",
    "    split_idx = int(len(reviews) * train_ratio)\n",
    "    train_indices, val_indices = indices[:split_idx], indices[split_idx:]\n",
    "    train_data = [reviews[i] for i in train_indices], sentiments[train_indices]\n",
    "    val_data = [reviews[i] for i in val_indices], sentiments[val_indices]\n",
    "    return train_data, val_data\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(reviews):\n",
    "    vocab = {}\n",
    "    for review in reviews:\n",
    "        for word in review.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)  # Start indices from 0\n",
    "    return vocab\n",
    "\n",
    "# Convert text to vector\n",
    "def text_to_vector(reviews, vocab, vector_dim=1000):\n",
    "    vectors = []\n",
    "    for review in reviews:\n",
    "        vector = np.zeros((vector_dim,))\n",
    "        for word in review.split():\n",
    "            if word in vocab:\n",
    "                vector[vocab[word] % vector_dim] += 1  # Simple hashing trick\n",
    "        vectors.append(vector)\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script\n",
    "file_path = \"C:/Users/beste/Downloads/c.csv\"\n",
    "reviews, sentiments = load_dataset(file_path)\n",
    "(train_reviews, train_labels), (val_reviews, val_labels) = split_dataset(reviews, sentiments)\n",
    "\n",
    "vocab = build_vocab(train_reviews)\n",
    "train_vectors = text_to_vector(train_reviews, vocab)\n",
    "val_vectors = text_to_vector(val_reviews, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beste\\AppData\\Local\\Temp\\ipykernel_9024\\3503500073.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_vectors = torch.tensor(train_vectors)\n",
      "C:\\Users\\beste\\AppData\\Local\\Temp\\ipykernel_9024\\3503500073.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_vectors = torch.tensor(val_vectors)\n",
      "C:\\Users\\beste\\AppData\\Local\\Temp\\ipykernel_9024\\3503500073.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_labels = torch.tensor(train_labels)\n",
      "C:\\Users\\beste\\AppData\\Local\\Temp\\ipykernel_9024\\3503500073.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_labels = torch.tensor(val_labels)\n"
     ]
    }
   ],
   "source": [
    "print(type(train_vectors))\n",
    "\n",
    "# convert to tensor\n",
    "train_vectors = torch.tensor(train_vectors)\n",
    "val_vectors = torch.tensor(val_vectors)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "print(type(train_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "one_hot(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Gradient descent\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     xenc \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_reviews\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m27\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;66;03m#input to the network one-hot encoding\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     logits \u001b[38;5;241m=\u001b[39m xenc \u001b[38;5;241m@\u001b[39m W \u001b[38;5;66;03m#predict log-counts\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     counts \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mexp() \u001b[38;5;66;03m#counts, equivalent to N\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: one_hot(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27),generator=g, requires_grad=True)\n",
    "\n",
    "# Gradient descent\n",
    "for k in range(100):\n",
    "\n",
    "    # Forward pass\n",
    "    xenc = F.one_hot(train_reviews, num_classes=27).float() #input to the network one-hot encoding\n",
    "    logits = xenc @ W #predict log-counts\n",
    "    counts = logits.exp() #counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True) #probabilities for next character\n",
    "    loss = -probs[torch.arange(num), train_labels].log().mean()\n",
    "    print(loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    W.grad = None #set to zero the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update\n",
    "    W.data += -10 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Define activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Neural Network\n",
    "class FeedForwardNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.W3 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "        self.W4 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b4 = np.zeros((1, output_size))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.A1 = sigmoid(self.Z1)\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        self.Z3 = np.dot(self.A1, self.W3) + self.b3\n",
    "        self.A3 = sigmoid(self.Z3)\n",
    "        return self.A2\n",
    "    \n",
    "    def backward(self, X, Y, learning_rate):\n",
    "        m = X.shape[0]\n",
    "        dZ2 = self.A2 - Y\n",
    "        dW2 = np.dot(self.A1.T, dZ2) / m\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "        dZ1 = np.dot(dZ2, self.W2.T) * sigmoid_derivative(self.Z1)\n",
    "        dW1 = np.dot(X.T, dZ1) / m\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "\n",
    "# Metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred >= 0.5))\n",
    "    fp = np.sum((y_true == 0) & (y_pred >= 0.5))\n",
    "    fn = np.sum((y_true == 1) & (y_pred < 0.5))\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "\n",
    "input_size = train_vectors.shape[1]\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "\n",
    "model = FeedForwardNN(input_size, hidden_size, output_size)\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Training with batches\n",
    "batch_size = 32\n",
    "num_batches = len(train_vectors) // batch_size\n",
    "\n",
    "for epoch in range(500):\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_vectors = train_vectors[start_idx:end_idx]\n",
    "        batch_labels = train_labels[start_idx:end_idx].reshape(-1, 1)\n",
    "        \n",
    "        predictions = model.forward(batch_vectors)\n",
    "        model.backward(batch_vectors, batch_labels, learning_rate)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        train_loss = -np.mean(train_labels * np.log(predictions) + (1 - train_labels) * np.log(1 - predictions))\n",
    "        print(f\"Epoch {epoch}, Loss: {train_loss}\")\n",
    "\n",
    "# Evaluation\n",
    "val_predictions = model.forward(val_vectors).flatten()\n",
    "precision, recall, f1_score = calculate_metrics(val_labels, val_predictions)\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1-score: {f1_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
